{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Copy of GNN_MuonE_v1_Robust_mc-recon_comparison.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcinwolter/Tracking_student2020/blob/master/GNN_MuonE_v1_Robust_mc_recon_comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_vum9p0R7yZ",
        "colab_type": "text"
      },
      "source": [
        "# Tutorial: GNNs for Particle Tracking\n",
        "\n",
        "HEP.TrkX group\n",
        "\n",
        "Steve Farrell, Daniel Murname\n",
        "\n",
        "*Feb 2020*\n",
        "\n",
        "Adapted for MuonE tracking by Marcin Wolter\n",
        "\n",
        "*July 2020*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZh8UpRMR7yp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "117ed738-d370-4d32-ccf6-9f7bea5e11b1"
      },
      "source": [
        "# System imports\n",
        "import os\n",
        "import sys\n",
        "from pprint import pprint as pp\n",
        "from time import time as tt\n",
        "\n",
        "# External imports\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCH4u73YSkuf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e5b57136-8b44-4c6f-a04d-208e010821cf"
      },
      "source": [
        "print(\"PyTorch version:\",print(torch.__version__),\", CUDA version:\", torch.version.cuda)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.5.1+cu101\n",
            "PyTorch version: None , CUDA version: 10.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yAAs3ZiTNGX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "59e84ef8-66f3-4bdb-f45f-5c7e60f90e88"
      },
      "source": [
        "!pip install torch-scatter==latest+cu101 torch-sparse==latest+cu101 -f https://s3.eu-central-1.amazonaws.com/pytorch-geometric.com/whl/torch-1.5.0.html\n",
        "!pip install torch-geometric"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://s3.eu-central-1.amazonaws.com/pytorch-geometric.com/whl/torch-1.5.0.html\n",
            "Collecting torch-scatter==latest+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_scatter-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl (12.2MB)\n",
            "\u001b[K     |████████████████████████████████| 12.3MB 251kB/s \n",
            "\u001b[?25hCollecting torch-sparse==latest+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_sparse-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl (21.6MB)\n",
            "\u001b[K     |████████████████████████████████| 21.6MB 1.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-sparse==latest+cu101) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy->torch-sparse==latest+cu101) (1.18.5)\n",
            "Installing collected packages: torch-scatter, torch-sparse\n",
            "Successfully installed torch-scatter-2.0.5 torch-sparse-0.6.6\n",
            "Collecting torch-geometric\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/18/93b190226d09958be96919fd50c55d28f83f1a1b9260a2b33499f9d86728/torch_geometric-1.6.0.tar.gz (172kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.5.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (4.41.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.22.2.post1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.48.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.0.5)\n",
            "Collecting rdflib\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/6b/6454aa1db753c0f8bc265a5bd5c10b5721a4bb24160fb4faf758cf6be8a1/rdflib-5.0.0-py3-none-any.whl (231kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 13.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.10.0)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.4)\n",
            "Collecting ase\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/70/a8b1a7831193aa228defd805891c534d3e4717c8988147522e673458ddce/ase-3.19.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 12.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.11.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torch-geometric) (0.16.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->torch-geometric) (4.4.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->torch-geometric) (0.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (49.1.0)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (0.31.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2020.6.20)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (2.4.7)\n",
            "Collecting isodate\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (1.12.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from ase->torch-geometric) (3.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->torch-geometric) (1.1.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ase->torch-geometric) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ase->torch-geometric) (0.10.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-1.6.0-cp36-none-any.whl size=296339 sha256=7deacd249da5a938b6710db067d4c986f81ec61ffbaedfac4bb9ae76e0d924f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/7f/33/acea5809d8580a7adf60dcd6d04f5fc50a7f983040f68be1ff\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: isodate, rdflib, ase, torch-geometric\n",
            "Successfully installed ase-3.19.1 isodate-0.6.0 rdflib-5.0.0 torch-geometric-1.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YLfHA3uSjL_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch_geometric.data import Data\n",
        "from torch_geometric.data import DataLoader\n",
        "import torch.nn as nn\n",
        "from torch_scatter import scatter_add\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact, interact_manual\n",
        "from IPython.display import clear_output\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Get rid of RuntimeWarnings, gross\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYwHU7mHR7zR",
        "colab_type": "text"
      },
      "source": [
        "## The Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB6mrc_KR71-",
        "colab_type": "text"
      },
      "source": [
        "The complexity of the graph depends on the angle cut we put on it. Try increasing max_angle to, say, `(5/6)*np.pi` and the graph should be more busy. The limit is of course `(6/6)*np.pi = pi` where each node will look at the full angle of available possible nodes to form an edge with. While playing with this number, run the next cell to see the proportion of fake edges to true edges (fake/true) on the above graph. This value will be extremely useful later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eusssWsdR73x",
        "colab_type": "text"
      },
      "source": [
        "## A Simple GNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1FRm4nsR730",
        "colab_type": "text"
      },
      "source": [
        "### Message Passing GNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gx0VuKz1R732",
        "colab_type": "text"
      },
      "source": [
        "We can write out the full GNN as a class `MPNN_Network`. One can see its behaviour as:\n",
        "1. Encode (x,y) features as hidden features with an N-layer MLP called `node_encoder`\n",
        "2. Concatenate these along each edge, and feed the concatenated features into another MLP called `edge_network`\n",
        "3. Sum the output of `edge_classifier` at each node (that is, each node receives the sum of the \"messages\" of all connecting edges). This sum is fed into `node_network`\n",
        "4. Add the hidden features to the previous iteration (this helps to preserve information between messages)\n",
        "5. Repeat (2) --> (4) n_graph_iters times\n",
        "6. After the message passing loop, pass the features of each edge through an output classifier network called `edge_classifier`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEPycV6mR734",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_mlp(input_size, sizes,\n",
        "             hidden_activation='ReLU',\n",
        "             output_activation='ReLU',\n",
        "             layer_norm=False):\n",
        "    \"\"\"Construct an MLP with specified fully-connected layers.\"\"\"\n",
        "\n",
        "    hidden_activation = getattr(nn, hidden_activation)\n",
        "    if output_activation is not None:\n",
        "        output_activation = getattr(nn, output_activation)\n",
        "    layers = []\n",
        "    n_layers = len(sizes)\n",
        "    sizes = [input_size] + sizes\n",
        "    # Hidden layers\n",
        "\n",
        "    for i in range(n_layers-1):\n",
        "        layers.append(nn.Linear(sizes[i], sizes[i+1]))\n",
        "        if layer_norm:\n",
        "            layers.append(nn.LayerNorm(sizes[i+1]))\n",
        "        layers.append(hidden_activation())\n",
        "\n",
        "    # Final layer\n",
        "    layers.append(nn.Linear(sizes[-2], sizes[-1]))\n",
        "    if output_activation is not None:\n",
        "        if layer_norm:\n",
        "            layers.append(nn.LayerNorm(sizes[-1]))\n",
        "        layers.append(output_activation())\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "class MPNN_Network(nn.Module):\n",
        "    \"\"\"\n",
        "    A message-passing graph network which takes a graph with:\n",
        "    - bi-directional edges\n",
        "    - node features, no edge features\n",
        "\n",
        "    and applies the following modules:\n",
        "    - a graph encoder (no message passing)\n",
        "    - recurrent edge and node networks\n",
        "    - an edge classifier\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_node_dim, hidden_edge_dim, in_layers, node_layers, edge_layers,\n",
        "                 n_graph_iters=1, layer_norm=True,normalize_factor= 1):\n",
        "        super(MPNN_Network, self).__init__()\n",
        "        self.n_graph_iters = n_graph_iters\n",
        "\n",
        "        # The node encoder transforms input node features to the hidden space\n",
        "        self.node_encoder = make_mlp(input_dim, [hidden_node_dim]*in_layers)\n",
        "\n",
        "        # The edge network computes new edge features from connected nodes\n",
        "        self.edge_network = make_mlp(2*hidden_node_dim,\n",
        "                                     [hidden_edge_dim]*edge_layers,\n",
        "                                     layer_norm=layer_norm)\n",
        "\n",
        "        # The node network computes new node features\n",
        "        self.node_network = make_mlp(hidden_node_dim + hidden_edge_dim,\n",
        "                                     [hidden_node_dim]*node_layers,\n",
        "                                     layer_norm=layer_norm)\n",
        "\n",
        "        # The edge classifier computes final edge scores\n",
        "        self.edge_classifier = make_mlp(2*hidden_node_dim,\n",
        "                                        [hidden_edge_dim, 1],\n",
        "                                        output_activation=None)\n",
        "        self.normalize_factor=normalize_factor\n",
        "    def forward(self, data):\n",
        "        # Make every edge bi-directional\n",
        "        send_idx = torch.cat([data.edge_index[0], data.edge_index[1]], dim=0)\n",
        "        recv_idx = torch.cat([data.edge_index[1], data.edge_index[0]], dim=0)\n",
        "        \n",
        "        # Encode the graph features into the hidden space\n",
        "        #print( torch.mean (data.x[:,0]) )\n",
        "        #print( torch.mean (data.x[:,1]) )\n",
        "        if self.normalize_factor!=1:\n",
        "          temp = data.x.clone().detach()\n",
        "          temp[:,1] *= self.normalize_factor\n",
        "          x = self.node_encoder(temp)\n",
        "        else:\n",
        "          x = self.node_encoder(data.x.clone().detach())\n",
        "        # Loop over graph iterations\n",
        "        for i in range(self.n_graph_iters):\n",
        "\n",
        "            # Previous hidden state\n",
        "            x0 = x\n",
        "\n",
        "            # Compute new edge features\n",
        "            edge_inputs = torch.cat([x[send_idx], x[recv_idx]], dim=1)\n",
        "            e = self.edge_network(edge_inputs)\n",
        "\n",
        "            # Sum edge features coming into each node\n",
        "            aggr_messages = scatter_add(e, recv_idx, dim=0, dim_size=x.shape[0])\n",
        "\n",
        "            # Compute new node features\n",
        "            node_inputs = torch.cat([x, aggr_messages], dim=1)\n",
        "            x = self.node_network(node_inputs)\n",
        "\n",
        "            # Residual connection\n",
        "            x = x + x0\n",
        "\n",
        "        # Compute final edge scores; use original edge directions only\n",
        "        start_idx, end_idx = data.edge_index\n",
        "        clf_inputs = torch.cat([x[start_idx], x[end_idx]], dim=1)\n",
        "        return self.edge_classifier(clf_inputs).squeeze(-1)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOsNvEEpR74O",
        "colab_type": "text"
      },
      "source": [
        "Build a version of the model and print it out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fPSN18uR74Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#m_configs = {\"input_dim\": 2, \"hidden_node_dim\": 64, \"hidden_edge_dim\": 64, \"in_layers\": 2, \"node_layers\": 4, \"edge_layers\": 4, \"n_graph_iters\": 1, \"layer_norm\": True}\n",
        "#model = MPNN_Network(**m_configs).to(device)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSM30cK2R746",
        "colab_type": "text"
      },
      "source": [
        "### Training and Validation Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQrAmWD23j0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, train_loader, optimizer):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        data = batch.to(device)\n",
        "        pred = model(data)\n",
        "        loss = F.binary_cross_entropy_with_logits(pred.float(), data.y.float(), pos_weight=torch.tensor(weight))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        correct += ((pred > 0.5) == (data.y > 0.5)).sum().item()\n",
        "        total += len(pred)\n",
        "    acc = correct/total\n",
        "    return acc, total_loss\n",
        "\n",
        "def evaluate(model, test_loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    total_loss = 0\n",
        "    for batch in test_loader:\n",
        "        data = batch.to(device)\n",
        "        pred = model(data)\n",
        "        loss = F.binary_cross_entropy_with_logits(pred.float(), data.y.float(), pos_weight=torch.tensor(weight))\n",
        "        total_loss += loss.item()\n",
        "        correct += ((pred > 0.5) == (data.y > 0.5)).sum().item()\n",
        "        total += len(pred)\n",
        "    acc = correct/total\n",
        "    return acc, total_loss"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wbNjVo3R75N",
        "colab_type": "text"
      },
      "source": [
        "We set a weight value that is more or less the (fake / true) ratio found above. This forces the loss function to punish incorrectly classified true edges more severely. It rebalances the distribution as if there was a 1:1 true:fake ratio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEDdFqVnR75x",
        "colab_type": "text"
      },
      "source": [
        "### Did it work?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fTa4sHsR75z",
        "colab_type": "text"
      },
      "source": [
        "Running the above with 1 graph iteration gives me about 90% accuracy in 200 epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lT5s0J70R751",
        "colab_type": "text"
      },
      "source": [
        "The best performance that I can get with some simple manual tuning is around 95% accuracy. We can improve the efficiency at the cost of purity by raising the weight on true edges, but the accuracy won't significantly improve. In general, the biggest changes were from increasing the width (i.e. the number of dimensions) of the hidden layers. We can visualise the performance on a particular graph, colouring true positives black, false positives red, true negatives a transparent black, and false negatives in blue."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6CR0cLcR752",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_toy_classified(event, preds, cut=0.5):\n",
        "    \n",
        "    #print(\"New event\")\n",
        "        \n",
        "    #print(event)\n",
        "    #print(event.x)\n",
        "    #print(event.pid)\n",
        "    #print(event.y)\n",
        "    #gprint(preds)\n",
        "\n",
        "    plt.figure(figsize=(10,5))\n",
        "    x, y = event.x[:,0].numpy(), event.x[:,1].numpy()\n",
        "    edges = event.edge_index.numpy()\n",
        "    labels = event.y\n",
        "    plt.scatter(x, y, c='k')\n",
        "    \n",
        "    preds = preds.detach().numpy()\n",
        "    \n",
        "\n",
        "    for j in range(len(labels)):\n",
        "        \n",
        "        #print(str('%01.2f' % preds[j])+\" \"+str(labels[j].item())+\" \"+str(x[edges[0,j]]*feature_scale)+\",\"+str(y[edges[0,j]]*feature_scale)+\" \"+str(x[edges[1,j]]*feature_scale)+\",\"+str(y[edges[1,j]]*feature_scale))\n",
        "        s = str('%01.2f' % preds[j])+\" \"+str(labels[j].item())\n",
        "        plt.text((x[edges[0,j]]+x[edges[1,j]])/2., (y[edges[0,j]]+y[edges[1,j]])/2.+np.random.random(1)*0.00, s, fontsize=12)\n",
        "\n",
        "        # False negatives\n",
        "        if preds[j] < cut and labels[j].item() > cut:\n",
        "            plt.plot([x[edges[0,j]], x[edges[1,j]]],\n",
        "                     [y[edges[0,j]], y[edges[1,j]]],\n",
        "                     '--', c='b')\n",
        "\n",
        "        # False positives\n",
        "        if preds[j] > cut and labels[j].item() < cut:\n",
        "            plt.plot([x[edges[0,j]], x[edges[1,j]]],\n",
        "                     [y[edges[0,j]], y[edges[1,j]]],\n",
        "                     '-', c='r', alpha=preds[j])\n",
        "\n",
        "        # True positives\n",
        "        if preds[j] > cut and labels[j].item() > cut:\n",
        "            plt.plot([x[edges[0,j]], x[edges[1,j]]],\n",
        "                     [y[edges[0,j]], y[edges[1,j]]],\n",
        "                     '-', c='k', alpha=preds[j])\n",
        "                \n",
        "        # True negatives\n",
        "        if preds[j] < cut and labels[j].item() < cut:\n",
        "            plt.plot([x[edges[0,j]], x[edges[1,j]]],\n",
        "                     [y[edges[0,j]], y[edges[1,j]]],\n",
        "                     '-', c='k', alpha=preds[j])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIo7EeyJR76L",
        "colab_type": "text"
      },
      "source": [
        "So we can see that it's working quite well. Few missed true edges, and few misclassified fake edges. The ratio of false positives to false negatives (which can be defined with efficiency and purity) is controlled by the cut we put on the prediction score. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CBDsYlxR76N",
        "colab_type": "text"
      },
      "source": [
        "### The effect of Message Passing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSZzzAtDR76P",
        "colab_type": "text"
      },
      "source": [
        "In this simple example, the message passing does not do a huge amount. Going from 1 iteration to 6 iterations improves the accuracy to around 93% (from 90%). This improvement is washed out with more hidden dimensions, as one can see from the below set of tests. Try playing with more hidden node and edge dimensions (e.g. 64) and see if the message passing iterations can improve the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7koxeu9q4Gn",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "# **Load MuonE data and reconstruct tracks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Vk18NYQOrfzB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "0f159da6-1355-4b0b-873d-be852feac23a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xZs_yxD3FRKf",
        "colab": {}
      },
      "source": [
        "# Circle parameters\n",
        "num_layers = 14\n",
        "height, width = 10, 10\n",
        "#min_curve, max_curve =  1000, 1001 # 15, 50\n",
        "\n",
        "max_angle = (0.05)*np.pi\n",
        "feature_scale = 200."
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgK9yW_ErILc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "  \n",
        "def readMuonE(event_file,is_x=True):\n",
        "\n",
        "\n",
        "   data = pd.read_csv(event_file) \n",
        "   # Preview the first 5 lines of the loaded data \n",
        "   data.head()\n",
        "\n",
        "  #if axis_oi == 0 we are getting the data from the X detector layers\n",
        "  #if axis oi == 1, it's about the data from Y detector layers\n",
        "   if is_x:\n",
        "    axis_oi=0\n",
        "\n",
        "   else:\n",
        "    axis_oi=1\n",
        "\n",
        "   \n",
        "   \n",
        "   \n",
        "   # Temporary nextZ  (next layer Z)\n",
        "   layerZ0 =  [-999, -16, -21, 131, 151, 477, 497, 832, 843.7, 1146, 1141, 1252.7, 1266, 1973, 1960 ]\n",
        "\n",
        "\n",
        "\n",
        "   #X = np.array([[0,0,0]])\n",
        "   #print(X)\n",
        "   \n",
        "   \n",
        "   iEventLast=data.iat[0,0]\n",
        "   k=0\n",
        "   Xcreated=False\n",
        "   #dataOUTcreated=False\n",
        "   dataOUT = []\n",
        "\n",
        "   #iterate over input data\n",
        "   for k, row in data.iterrows():\n",
        "     ll=data.iat[k,0]\n",
        "     #print(\"counter \",k,ll,iEventLast)\n",
        "     if ll != iEventLast:\n",
        "\n",
        "       '''\n",
        "       for layer in np.arange(num_layers-1):\n",
        "          print(\"XX=\",layer,X)\n",
        "          for i in np.argwhere(X[:,2] == layerZ0[layer+1]): \n",
        "            for j in np.argwhere(X[:,2] == layerZ0[layer+3]):\n",
        "               if (X[i, 0] - 20. < X[j, 0] < X[i, 0] + 20.):\n",
        "                   print(\"layer, i, j=\",layer, i, j)\n",
        "       '''\n",
        "#       e = np.array([[i,j] for layer in np.arange(num_layers-1) for i in np.argwhere(X[:,2] == layer+1) for j in np.argwhere(X[:,2] == (layer+2))  if (X[i, 0] - np.tan(max_angle/2) < X[j, 0] < X[i, 0] + np.tan(max_angle/2))]).T.squeeze()\n",
        "\n",
        "       e = np.array([[i,j] for layer in np.arange(num_layers-1) for i in np.argwhere(X[:,2] == \n",
        "              layerZ0[layer+1]) for j in np.argwhere(X[:,2] == layerZ0[layer+3])  if (X[i, 0] - np.tan(max_angle/2)*(layerZ0[layer+3]-layerZ0[layer+1]) < X[j, 0] < X[i, 0] + np.tan(max_angle/2)*(layerZ0[layer+3]-layerZ0[layer+1]))]).T.squeeze()\n",
        "\n",
        "       #print(X)\n",
        "       #print(\"e \",e) \n",
        "        # This handles when no edges were constructed. In that case, the randomisation is a do-over\n",
        "       #try:\n",
        "       y = np.array([int(i[1] == j[1]) for i,j in zip(X[e[0]], X[e[1]])])\n",
        "       #     print(\"y = \",y)     \n",
        "       #     break\n",
        "       #except:\n",
        "       #     pass\n",
        "\n",
        "       #print(\"y = \",y) \n",
        "       #if iter is not None and num_samples is not None:\n",
        "       #     out.update(progress(iter, num_samples))    \n",
        "\n",
        "\n",
        "       X = np.array([X[:,2], X[:,0]]).T / feature_scale\n",
        "\n",
        "       dataEvent = Data(x = torch.from_numpy(X).float(), edge_index = torch.from_numpy(e), y = torch.from_numpy(y), pid = torch.from_numpy(X[:,1])  )\n",
        "\n",
        "       #plot_toy_graph(dataEvent, 0.2)\n",
        "\n",
        "       dataOUT.append(dataEvent)\n",
        "#       if dataOUTcreated:\n",
        "#         dataOUT = dataOUT.append(dataEvent)\n",
        "#       else:\n",
        "#         dataOUT = dataEvent \n",
        "#         dataOUTcreated = True\n",
        "\n",
        "       Xcreated = False  \n",
        "       iEventLast = ll\n",
        "\n",
        "     #print(len(X)) \n",
        "     #print(k,ll,data.iat[k,2],Xcreated) \n",
        "     if data.iat[k,2]==axis_oi and data.iat[k,5]<3 and data.iat[k,5]>=0: #remove equal to part if you don't want to  # select X-hit and trackID=0,1,2\n",
        "        if Xcreated:\n",
        "          X = np.append(X,np.array([[data.iat[k,1], data.iat[k,5], data.iat[k,3]  ]]),axis=0)\n",
        "        else:\n",
        "          X = np.array([[data.iat[k,1], data.iat[k,5], data.iat[k,3]  ]]) \n",
        "          Xcreated=True\n",
        "          #print(\"X created \",X)\n",
        "     \n",
        "   return dataOUT       \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Reads original monte carlo File\n",
        "\n",
        "def readMCtrack(event_file, is_x= True ):\n",
        "\n",
        "  data = pd.read_csv(event_file) \n",
        "  # Preview the first 5 lines of the loaded data \n",
        "  data.head()\n",
        "\n",
        "  if is_x:\n",
        "    axis_oi=[4,1]\n",
        "  else  :\n",
        "    axis_oi=[5,2]\n",
        "\n",
        "  temp=data.iat[-1,0]\n",
        "  track_slopes=torch.zeros(temp,6)\n",
        "\n",
        "  for k, row in data.iterrows():\n",
        "\n",
        "    if (data.iat[k,7]==np.array([0,1,2])).any():\n",
        "\n",
        "      \n",
        "\n",
        "      if track_slopes.shape[0]< data.iat[k,0]-1 :\n",
        "        track_slopes=torch.cat([track_slopes,torch.zeros(1,6)],dim=0)\n",
        "      \n",
        "      \n",
        "      track_slopes[data.iat[k,0]-1,data.iat[k,7]*2 ] = data.iat[k,axis_oi[0]]/data.iat[k,6]\n",
        "\n",
        "      track_slopes[data.iat[k,0]-1,data.iat[k,7]*2+1] =  data.iat[k,axis_oi[1]] - data.iat[k,axis_oi[0]]/data.iat[k,6]  * data.iat[k,3]\n",
        "  \n",
        "  return track_slopes\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Reads From Reconstruction File\n",
        "\n",
        "def ReadRecon(event_file,is_x=True):\n",
        "  data = pd.read_csv(event_file)  \n",
        "  data.head()\n",
        "\n",
        "  if is_x:\n",
        "    axis_oi=[3,1,7,5,9]\n",
        "  else:\n",
        "    axis_oi=[4,2,8,6,9]\n",
        "\n",
        "  track_=torch.zeros(data.iat[-1,0],10)\n",
        "\n",
        "  for k,row in enumerate(data.iterrows()):\n",
        "\n",
        "    if k % 2 ==0:\n",
        "      #track_[data.iat[k,0]-1,0],track_[data.iat[k,0]-1,1]=data.iat[k,axis_oi[0]],data.iat[k,axis_oi[1]]\n",
        "      for i in range( len(axis_oi) ):\n",
        "        track_[data.iat[k,0]-1,i]=data.iat[k,axis_oi[i]]\n",
        "    else:\n",
        "      #track_[data.iat[k,0]-1 ,2],track_[data.iat[k,0]-1,3]=data.iat[k,axis_oi[0]],data.iat[k,axis_oi[1]]\n",
        "      for i in range( len(axis_oi) ):\n",
        "        track_[data.iat[k,0]-1,i+5]=data.iat[k,axis_oi[i]]\n",
        "  print(k)\n",
        "  \n",
        "\n",
        "  temp=track_.clone()\n",
        "  for i in range(data.iat[-1,0]):\n",
        "    if torch.abs(temp[i,0])>torch.abs(temp[i,5]):\n",
        "\n",
        "      track_[i,0:4]=temp[i,5:9]\n",
        "      track_[i,5:9]=temp[i,0:4]\n",
        "\n",
        "\n",
        "\n",
        "  return track_\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zhaE3IHvLwu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "c6993726-ebbb-4bd9-eef4-d7a6506ea8ae"
      },
      "source": [
        "# Read data and prepare for training\n",
        "\n",
        "is_x = True\n",
        "is_y = False\n",
        "event_file_MC= \"/content/drive/My Drive/MuonE_tracking/MCtrackFile_medium.csv\"\n",
        "event_file= \"/content/drive/My Drive/MuonE_tracking/hitFile_medium.csv\"\n",
        "event_file_recon= \"/content/drive/My Drive/MuonE_tracking/trackFile_medium.csv\"\n",
        "\n",
        "Tracks=readMCtrack(event_file_MC,is_x)\n",
        "data = readMuonE(event_file,is_x)\n",
        "Tracks_recon=ReadRecon(event_file_recon,is_x)\n",
        "\n",
        "#Tracks_x=readMCtrack(event_file_MC,is_x)\n",
        "#data_x = readMuonE(event_file,is_x)\n",
        "#Tracks_recon_x=ReadRecon(event_file_recon,is_x)\n",
        "\n",
        "# split data into training and test datasets\n",
        "train_dataset,test_dataset,test_tracks,train_tracks,train_recon,test_recon = [],[],[],[],[],[]\n",
        "\n",
        "for k in range(len(data)):\n",
        "  if (k%4 == 0):\n",
        "    test_dataset.append(data[k])\n",
        "    test_tracks.append(Tracks[k,:])\n",
        "    test_recon.append(Tracks_recon[k,:])\n",
        "  else:\n",
        "    train_dataset.append(data[k])\n",
        "    train_tracks.append(Tracks[k,:])\n",
        "    train_recon.append(Tracks_recon[k,:])\n",
        "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=True)    \n",
        "\n",
        "# Display few events\n",
        "#for k in range(10):\n",
        " #  plot_toy_graph(train_loader.dataset[k], 0.2)\n",
        "\n",
        "\n",
        "print(\"Fake / True = \", (len(train_dataset[0].y) - train_dataset[0].y.sum().item()) / train_dataset[0].y.sum().item())\n",
        "print(\"Fake / True = \", (len(test_dataset[0].y) - test_dataset[0].y.sum().item()) / test_dataset[0].y.sum().item())\n",
        "\n",
        "print(\"Training dataset (events): \",len(train_dataset))\n",
        "print(\"Test dataset (events): \",len(test_dataset))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19073\n",
            "Fake / True =  1.0\n",
            "Fake / True =  0.875\n",
            "Training dataset (events):  7499\n",
            "Test dataset (events):  2500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35sTNGChGhQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_toy_graph(event, ylim=0.):\n",
        "  \n",
        "    #print(event)\n",
        "    #print(event.x)\n",
        "    #print(event.pid)\n",
        "    #print(event.y)\n",
        "\n",
        "    plt.figure(figsize=(10,5))\n",
        "    x, y = event.x[:,0].numpy(), event.x[:,1].numpy()\n",
        "    e = event.edge_index.numpy()\n",
        "#     for i, j in zip(X[e[0]], X[e[1]]):\n",
        "#         plt.plot([i[0], j[0]], [i[1], j[1]], c='b')\n",
        "    #     print(i[0], i[2], j[0], j[2])\n",
        "    plt.plot([x[e[0,:]], x[e[1,:]]], [y[e[0,:]], y[e[1,:]]], c='b')\n",
        "    plt.scatter(x, y, c='k')\n",
        "    if (ylim!=0):\n",
        "       plt.ylim(-ylim,ylim)\n",
        "    else:\n",
        "       diff = 0.1*(max(y)-min(y))\n",
        "       plt.ylim(min(y)-diff, max(y)+diff)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "recqp28g1f1Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Display few events\n",
        "for k in range(10):\n",
        "   plot_toy_graph(train_loader.dataset[k], 0.)\n",
        "\n",
        "print(train_loader.dataset[k].x)\n",
        "\n",
        "print(\"Fake / True = \", (len(train_dataset[0].y) - train_dataset[0].y.sum().item()) / train_dataset[0].y.sum().item())\n",
        "\n",
        "print(\"Training dataset (events): \",len(train_dataset))\n",
        "print(\"Test dataset (events): \",len(test_dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6jkuybVoBqw",
        "colab_type": "text"
      },
      "source": [
        "# **Simple GNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vj7GJA-_okqv"
      },
      "source": [
        "We set a weight value that is more or less the (fake / true) ratio found above. This forces the loss function to punish incorrectly classified true edges more severely. It rebalances the distribution as if there was a 1:1 true:fake ratio."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2qlRvrHbokqw",
        "colab": {}
      },
      "source": [
        "t_loss_v = []\n",
        "t_acc_v = []\n",
        "v_loss_v = []\n",
        "v_acc_v = []\n",
        "ep = 0\n",
        "\n",
        "weight = 1\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#device = torch.device('cpu')\n",
        "m_configs = {'normalize_factor': 2000 ,\"input_dim\": 2, \"hidden_node_dim\": 64, \"hidden_edge_dim\": 64, \"in_layers\": 1, \"node_layers\": 2, \"edge_layers\": 2, \"n_graph_iters\": 6, \"layer_norm\": True}\n",
        "model = MPNN_Network(**m_configs).to(device)\n",
        "\n",
        "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#m_configs = {\"input_dim\": 2, \"hidden_node_dim\": 64, \"hidden_edge_dim\": 64, \"in_layers\": 2, \"node_layers\": 4, \"edge_layers\": 4, \"n_graph_iters\": 1, \"layer_norm\": True}\n",
        "#model = MPNN_Network(**m_configs).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-3, amsgrad=True)\n",
        "\n",
        "for epoch in range(52):\n",
        "    ep += 1  \n",
        "    model.train()\n",
        "    acc_t, total_loss = train(model, train_loader, optimizer)\n",
        "    t_loss_v.append(total_loss)\n",
        "    t_acc_v.append(acc_t)\n",
        "\n",
        "    model.eval()\n",
        "    acc, total_loss = evaluate(model, test_loader)\n",
        "    v_loss_v.append(total_loss)\n",
        "    v_acc_v.append(acc)\n",
        "    if ep % 5 ==1 :\n",
        "      print('Epoch: {}, Accuracy: {:.4f}'.format(ep, acc,acc_t))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qgTQxer7okq8",
        "colab": {}
      },
      "source": [
        "fig, axs = plt.subplots(1, 2, constrained_layout=True, figsize=(10, 5))\n",
        "\n",
        "axs[0].plot(np.arange(len(t_loss_v)), t_loss_v)\n",
        "axs[0].twinx().plot(np.arange(len(t_acc_v)), t_acc_v,\"r\")\n",
        "axs[0].set_title(\"Training loss and accuracy\")\n",
        "axs[0].set_yscale(\"linear\")\n",
        "axs[1].plot(np.arange(len(v_loss_v)), v_loss_v)\n",
        "axs[1].twinx().plot(np.arange(len(v_acc_v)), v_acc_v,\"r\")\n",
        "axs[1].set_title(\"Validation loss and accuracy\")\n",
        "axs[1].set_yscale(\"linear\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ry91JU8tokrG"
      },
      "source": [
        "### Did it work?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ti-OFV5cokrM",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "for i in range(0,1500):\n",
        "  data = test_loader.dataset[i].to(device)\n",
        "  preds = torch.sigmoid(model(data)).to('cpu')  \n",
        "  \n",
        "  a=0\n",
        "  for j in range(preds.shape[0]):\n",
        "    if preds[j] > 0.5 and preds[j] < 0.8 and 0 :   \n",
        "      a=1\n",
        "  if a==1:\n",
        "    plot_toy_classified(data.to('cpu'), preds, cut = 0.6)\n",
        "  \n",
        "\n",
        "  #plot_toy_graph(train_loader.dataset[2],0.2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sq3gU_arP2S6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import (\n",
        "    LinearRegression, TheilSenRegressor, RANSACRegressor, HuberRegressor)\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn import linear_model\n",
        "\n",
        "from scipy.optimize import curve_fit, least_squares\n",
        "from scipy import stats\n",
        "\n",
        "\n",
        "def finder(data,y,cut=0.5):\n",
        "  x = data.x.clone().numpy()\n",
        "  edge_index=data.edge_index.detach().numpy()\n",
        "  node_identifier=np.zeros( list(x.shape)[0] )\n",
        "  k=edge_index.shape[1]\n",
        "  y=y.detach().tolist()\n",
        "  is_connected=[]\n",
        "  for i in range(k):\n",
        "    if y[i]-cut>=0:\n",
        "      is_connected.append( True )\n",
        "    else:\n",
        "      is_connected.append( False )\n",
        "\n",
        "  for j in range(1,4):\n",
        "\n",
        "    a=np.where( node_identifier==0 )\n",
        "    if j==1:\n",
        "      try:\n",
        "        min=np.min(np.abs(x[a,0]))\n",
        "      except:\n",
        "        print('oopsie')\n",
        "        break\n",
        "      idx=np.where(np.abs(x[:,0])==min)\n",
        "    \n",
        "\n",
        "    else:\n",
        "      try:\n",
        "        min=np.min(np.abs(x[a,1]))\n",
        "      except:\n",
        "        print('oopsie')\n",
        "        break\n",
        "      idx=np.where(np.abs(x[:,1])==min)\n",
        "    \n",
        "    try:\n",
        "      idx=idx[0]\n",
        "    except:\n",
        "      pass\n",
        "    \n",
        "    neighbor = 1\n",
        "    node_identifier[idx] = j\n",
        "    \n",
        "    \n",
        "    while neighbor :\n",
        "\n",
        "      neighbor = []\n",
        "      try:\n",
        "        idx=idx[-1]\n",
        "      except:\n",
        "        pass\n",
        "      in_neighbor = [ int(edge_index[0,i]) for i in range(k) if int(edge_index[1,i])==idx if is_connected[i] \n",
        "                     if node_identifier[ edge_index[0,i] ] ==0  ]\n",
        "\n",
        "      ot_neighbor = [ int(edge_index[1,i]) for i in range(k) if int(edge_index[0,i])==idx if is_connected[i] \n",
        "                     if node_identifier[ edge_index[1,i] ] ==0  ]\n",
        "      neighbor = in_neighbor + ot_neighbor\n",
        "      \n",
        "      if len(neighbor)>0:\n",
        "        for i in reversed( range( len(neighbor) ) ):\n",
        "          if node_identifier[neighbor[i]] != 0 :\n",
        "            del neighbor[i]\n",
        "      \n",
        "\n",
        "      if neighbor:\n",
        "        idx=neighbor[0]\n",
        "\n",
        "      node_identifier[neighbor] = j\n",
        "\n",
        "\n",
        "\n",
        "  return node_identifier\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def fit(x,a,b): \n",
        "  return a*x+b\n",
        "def chi2(x,y,yerr,fit,popt):\n",
        "  # Now calculate chi square \n",
        "  yexp = fit(x, *popt)\n",
        "  r_ = (y - yexp)/yerr\n",
        "  chisq_ = np.sum(r_**2)\n",
        "  df_ = len(x) - 2\n",
        "  return chisq_, df_, r_\n",
        "\n",
        "\n",
        "\n",
        "def robust_fit(data,preds,toleration=0.5):\n",
        "  node_lines = finder(data,preds)\n",
        "  x,y=data.x[:,0].clone().numpy()*feature_scale,data.x[:,1].clone().numpy()*feature_scale\n",
        "  \n",
        "  parameters=[]\n",
        "  errors=[]\n",
        "  popt=[]\n",
        "  pcov=[]\n",
        "  for i in range(2,4):\n",
        "    \n",
        "    nodes=np.where(node_lines==i)\n",
        "    \n",
        "    if np.size(nodes)>1 :\n",
        "\n",
        "      yerr = np.ones( np.size(nodes) )*0.03\n",
        "      popt,pcov=curve_fit(fit,x[nodes],y[nodes],p0=(0.0,0.0),sigma=yerr, absolute_sigma=True) \n",
        "\n",
        "      a=popt[0]\n",
        "      err_a=np.sqrt(pcov[0,0])\n",
        "      b=popt[1]\n",
        "      err_b=np.sqrt(pcov[1,1])\n",
        "      chisq_limit = toleration \n",
        "\n",
        "      chisq, df, r = chi2(x[nodes],y[nodes],yerr,fit,popt)\n",
        "      chisq_=[]\n",
        "      if int(chisq/df > chisq_limit) and np.size(nodes)>3  :\n",
        "        for ii in range(np.size(nodes)):\n",
        "\n",
        "          x_robust = np.delete(x[nodes],ii)\n",
        "          y_robust = np.delete(y[nodes],ii)\n",
        "          yerrii = np.delete(yerr,ii)\n",
        "          popt,pcov=curve_fit(fit , x_robust , y_robust , p0=(0.0,0.0),sigma=yerrii, absolute_sigma=True)\n",
        "          chisq, df, r = chi2(x_robust,y_robust,yerrii,fit,popt)      \n",
        "          chisq_.append(chisq)\n",
        "\n",
        "        ii=np.where(np.array(chisq_)==np.amin(np.array(chisq_)))[0] \n",
        "        \n",
        "        x_robust = np.delete(x[nodes],ii)\n",
        "        y_robust = np.delete(y[nodes],ii)\n",
        "        yerr = np.delete(yerr,ii)\n",
        "        popt,pcov=curve_fit(fit , x_robust , y_robust , p0=(0.0,0.0),sigma=yerr, absolute_sigma=True)\n",
        "        chisq, df, r = chi2(x_robust,y_robust,yerr,fit,popt)      \n",
        "          \n",
        "\n",
        "\n",
        "\n",
        "      #if int(chisq/df > chisq_limit) and np.size(nodes)>3  :\n",
        "    # Find outlier and remove it\n",
        "\n",
        "    # find point with the highest abs(r)\n",
        "        #ii = np.where( np.abs(r) == np.amax(abs(r)))[0]\n",
        "\n",
        "        #x_robust = np.delete(x[nodes],ii)\n",
        "        #y_robust = np.delete(y[nodes],ii)\n",
        "        #yerr = np.delete(yerr,ii)\n",
        "    # repeat fit without outlier\n",
        "        #popt,pcov=curve_fit(fit , x_robust , y_robust , p0=(0.0,0.0),sigma=yerr, absolute_sigma=True)\n",
        "        #chisq, df, r = chi2(x_robust,y_robust,yerr,fit,popt)      \n",
        "\n",
        "\n",
        "      a=popt[0]\n",
        "      err_a=np.sqrt(pcov[0,0])\n",
        "      b=popt[1]\n",
        "      err_b=np.sqrt(pcov[1,1])    \n",
        "      parameters.append([a,b])\n",
        "      errors.append([err_a,err_b,yerr,chisq])\n",
        "\n",
        "    \n",
        "    else :\n",
        "      errors.append('N/A')\n",
        "      parameters.append('N/A')\n",
        "\n",
        "  return parameters,errors,node_lines\n",
        "\n",
        "\n",
        "\n",
        "def plot_classified_line(data,preds,Track,trackre ):\n",
        "\n",
        "  parameters_,errors_,node_lines=robust_fit(data,preds)\n",
        "  x,y=data.x[:,0]*feature_scale,data.x[:,1]*feature_scale\n",
        "  colors={'1':'r','2':'g','3':'b' }\n",
        "  fig=plt.figure(figsize=(10,10))\n",
        "  ax=fig.add_subplot(111)\n",
        "\n",
        "  if (node_lines==0).any()== True:\n",
        "    nodes=np.where(node_lines==0)\n",
        "    plt.plot(x[nodes],y[nodes], 'k'+'o' , label = 'Not Track')\n",
        "    \n",
        "  for i in range(1,4):\n",
        "    \n",
        "    nodes=np.where(node_lines==i)\n",
        "    plt.plot(x[nodes],y[nodes],colors[str(i)] +'o' )\n",
        "    nodes=np.where(node_lines==i)\n",
        "\n",
        "\n",
        "  for i in [2,3]:\n",
        "    \n",
        "    nodes=np.where(node_lines==i)\n",
        "    \n",
        "\n",
        "\n",
        "    if np.size(nodes)>1 :\n",
        "      \n",
        "      error=errors_[i-2]  \n",
        "      \n",
        "\n",
        "      x_guess=np.linspace(0,torch.max(x[nodes]),100)\n",
        "      x_line=np.linspace(torch.min(x[nodes]),torch.max(x[nodes]),100)\n",
        "\n",
        "      \n",
        "      \n",
        "      \n",
        "      slope_gnn=parameters_[i-2][0]\n",
        "      c_gnn=parameters_[i-2][1] \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      slope_recon_ = trackre[5*i-10]\n",
        "      c_recon = trackre[5*i-9]  \n",
        "      error_recon_ = trackre[5*i-8]\n",
        "      \n",
        "\n",
        "\n",
        "      slope_mc = Track[2*i-2]\n",
        "      #c_mc = Track[2*i-1]\n",
        "      c_mc=y[nodes][1]-slope_mc*x[nodes][1]\n",
        "\n",
        "      # Plotting of the GNN\n",
        "      \n",
        "\n",
        "      \n",
        "      plt.plot(x_guess,c_gnn + torch.tensor(x_guess)*slope_gnn,'--'+colors[str(i)],label=\"GNN{} Error: {:e}, \".format(i-1, \n",
        "          torch.abs( np.round( ( slope_mc - slope_gnn ) /error[0] ,2) ).tolist() ))\n",
        "      \n",
        "\n",
        "\n",
        "      # Plotting of original MC tracks\n",
        "\n",
        "\n",
        "      \n",
        "      \n",
        "      plt.plot(x_guess,c_mc + torch.tensor(x_guess)*slope_mc,'k',label='MC Track', alpha=0.4) \n",
        "\n",
        "      # Plotting of the regular reconstruction\n",
        "\n",
        "      \n",
        "      plt.plot( torch.tensor(x_line), torch.tensor(x_line) * slope_recon_ + c_recon ,colors[str(i)], label=r\"recon{} Error: {:e}, \".format(i-1, \n",
        "          torch.abs( np.round( (slope_mc-slope_recon_)/error_recon_ ,2) ).tolist() )  )   \n",
        "      \n",
        "\n",
        "            \n",
        "      ax.errorbar(x[nodes],y[nodes],yerr= np.ones(np.size(nodes,1) )*0.03,fmt='none', ecolor=colors[str(i)] )\n",
        "  plt.legend()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8gsImHWJeRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for i in range(len(train_loader.dataset)):\n",
        "for i in range(2,10):\n",
        "\n",
        "  data = train_loader.dataset[i].to(device) \n",
        "  preds=torch.sigmoid(model(data))\n",
        "  errors=robust_fit(data.to('cpu'),preds)[0]\n",
        "  a = errors[1][0]\n",
        "  b = errors[0][0]\n",
        "  if not (isinstance (a,str) or isinstance (b,str) ):\n",
        "    if True or b > 0.04 or b < -0.03 or a > 0.04 or a < -0.03  :\n",
        "      plot_classified_line(data.to('cpu'),preds,train_tracks[i], train_recon[i] )\n",
        "      plt.title(str(i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdAVIGR3ZZ6L",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGWEhVoPY2tg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.stats import norm\n",
        "\n",
        "error_recon,error_gnn,error_recon_nopull,error_gnn_nopull=[],[],[],[]\n",
        "\n",
        "for iter in range (0,len(test_tracks)):\n",
        "\n",
        "  data = test_loader.dataset[iter].to(device)\n",
        "  preds = torch.sigmoid(model(data))\n",
        "  temp = robust_fit(data.to('cpu'),preds)\n",
        "  slope = temp[0]\n",
        "\n",
        "  if not (slope[0] == 'N/A' or slope[1] == 'N/A' ) :\n",
        "\n",
        "    error_gnn.append( ((test_tracks[iter][2]-slope[0][0])/temp[1][0][0] )  )\n",
        "    error_gnn.append( ((test_tracks[iter][4]-slope[1][0])/temp[1][1][0] ))\n",
        "    \n",
        "    error_gnn_nopull.append( ((test_tracks[iter][2]-slope[0][0]) ).numpy() )\n",
        "    error_gnn_nopull.append( ((test_tracks[iter][4]-slope[1][0]) ).numpy() )\n",
        "  \n",
        "  error_recon.append( ((test_tracks[iter][2]-test_recon[iter][0]))/test_recon[iter][2]  )\n",
        "  error_recon.append( ((test_tracks[iter][4]-test_recon[iter][5]))/test_recon[iter][7] )\n",
        "\n",
        "  error_recon_nopull.append( ((test_tracks[iter][2]-test_recon[iter][0])) )\n",
        "  error_recon_nopull.append( ((test_tracks[iter][4]-test_recon[iter][5])))\n",
        "\n",
        "\n",
        "fig=plt.figure(figsize=(15,15))\n",
        "\n",
        "limit_1=0.001\n",
        "limit_2=4\n",
        "error_list = { 'Reconstructed,  True-Predicted ' : error_recon_nopull,'GNN, True-Predicted ' : error_gnn_nopull,\n",
        "              'Reconstructed, (True-Predicted)/error' : error_recon,'GNN, (True-Predicted)/error' : error_gnn}            \n",
        "\n",
        "\n",
        "\n",
        "for i,[title,fitdata] in enumerate(error_list.items()):\n",
        "\n",
        "  ax_list=fig.add_subplot(2,2,i+1)\n",
        "  if i<2:\n",
        "    limit=limit_1\n",
        "  else:\n",
        "    limit=limit_2\n",
        "  supress_=ax_list.hist(np.array(fitdata),bins=np.linspace(-limit,limit,100).tolist(), density=True)\n",
        "\n",
        "  if i>1:\n",
        "    #fit gaussian\n",
        "    #remove infinites from fitdata \n",
        "    fitdata2 = []\n",
        "    for k in range(len(fitdata)):\n",
        "      if fitdata[k].item()<2 and fitdata[k].item()>-2:\n",
        "        fitdata2.append(fitdata[k].item())\n",
        "\n",
        "    mean,std=norm.fit(fitdata2)\n",
        "    print(mean,std,fitdata2)\n",
        "    xmin, xmax = plt.xlim()\n",
        "    x = np.linspace(xmin, xmax, 100)\n",
        "    y = norm.pdf(x, mean, std) \n",
        "    plt.plot(x, y)\n",
        "    title = title+\" mean = %.2f,  std = %.2f\" % (mean, std)\n",
        "    plt.title(title)\n",
        "  else:\n",
        "    plt.title(title)\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "fig.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSuNZhYJPTgL",
        "colab_type": "text"
      },
      "source": [
        "Don't Run! Old Robust fit algorithm\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KukSaal_PYhY",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "#@title Default title text\n",
        "print(fdgldfc)\n",
        "def robust_fit(data,preds,estimator=linear_model.RANSACRegressor() ):\n",
        "  node_lines = finder(data,preds)\n",
        "  x,y=data.x[:,0],data.x[:,1]\n",
        "  errors,models=[],[]\n",
        "  parameters=[]\n",
        "  for i in range(1,4):\n",
        "    \n",
        "    nodes=np.where(node_lines==i)\n",
        "    \n",
        "    if np.size(nodes)>1 :\n",
        "      \n",
        "      model = linear_model.RANSACRegressor()\n",
        "      model.fit( x[nodes].reshape(-1, 1)  , y[nodes] )\n",
        "      \n",
        "      parameters.append(  [ model.estimator_.coef_, model.predict(  np.zeros(1).reshape(-1,1) ) ]) \n",
        "      error = mean_squared_error( model.predict(x[nodes].reshape(-1, 1)), y[nodes] )\n",
        "      error= np.round(error,2)\n",
        "      errors.append( error )\n",
        "      models.append( model )\n",
        "    \n",
        "    else  :\n",
        "\n",
        "      models.append('N/A')\n",
        "      errors.append('N/A')\n",
        "      parameters.append('N/A')\n",
        "  \n",
        "  return [models,errors,parameters,node_lines]\n",
        "\n",
        "def plot_classified_line(data,preds,Track,trackre ):\n",
        "\n",
        "  models,errors,parameters,node_lines=robust_fit(data,preds,linear_model.RANSACRegressor())\n",
        "  x,y=data.x[:,0],data.x[:,1]\n",
        "  colors={'1':'r','2':'g','3':'b' }\n",
        "  fig=plt.figure(figsize=(10,10))\n",
        "  ax=fig.add_subplot(111)\n",
        "\n",
        "  if (node_lines==0).any()== True:\n",
        "    nodes=np.where(node_lines==0)\n",
        "    plt.plot(x[nodes],y[nodes], 'k'+'o' , label = 'Not Track')\n",
        "  for i in range(1,4):\n",
        "    \n",
        "    nodes=np.where(node_lines==i)\n",
        "    plt.plot(x[nodes],y[nodes],colors[str(i)] +'o' )\n",
        "\n",
        "  for i in [2,3]:\n",
        "    \n",
        "    nodes=np.where(node_lines==i)\n",
        "    \n",
        "\n",
        "\n",
        "    if np.size(nodes)>1 :\n",
        "      \n",
        "      model=models[i-1]\n",
        "      error=errors[i-1]\n",
        "      \n",
        "      x_guess=np.linspace(0,torch.max(x[nodes]),100)\n",
        "      y_guess = model.predict( x_guess[:, np.newaxis] )\n",
        "      x_line=np.linspace(torch.min(x[nodes]),torch.max(x[nodes]),100)\n",
        "\n",
        "      slope_mc=Track[i-1]\n",
        "      c_mc = y[nodes][0] - x[nodes][0] * slope_mc\n",
        "      slope_recon_ = trackre[2*i-4]\n",
        "      c_recon= y[nodes][0] - x[nodes][0] * slope_recon_ \n",
        "\n",
        "      plt.plot(x_guess,y_guess,'--'+colors[str(i)],label=\"GNN{} Error: {:e}, \".format(i-1, \n",
        "          torch.abs( np.round( (slope_mc-parameters[i-1][0])/slope_mc ,2) ).tolist()[0] ))\n",
        "      \n",
        "\n",
        "      plt.plot( torch.tensor(x_line), torch.tensor(x_line) * slope_recon_ + c_recon ,colors[str(i)], label=\"recon{} Error: {:e}, \".format(i-1, \n",
        "          torch.abs( np.round( (slope_mc-slope_recon_)/slope_mc ,2) ).tolist() )  )   \n",
        "\n",
        "#. format( error , np.round(parameters[i-1][0],2),np.round(parameters[i-1][1],2)   )   )\n",
        "      \n",
        "      \n",
        "      plt.plot(x_line,c_mc + torch.tensor(x_line)*slope_mc,'k',label='MC Track', alpha=0.4) \n",
        "      \n",
        "            \n",
        "      #ax.errorbar(x[nodes],y[nodes],yerr= np.ones(np.size(nodes,1) )*0.03,fmt='none', ecolor=colors[str(i)] )\n",
        "  plt.legend()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdBGjDLpjfpy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_tracks[1008]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}